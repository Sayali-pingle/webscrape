import json
import datetime
import os

class MemoryWithLogging:
    def __init__(self, memory_file='memory.json', log_file='logs.json'):
        self.memory_file = memory_file
        self.log_file = log_file
        self.memory = self.load_memory()
        self.logs = self.load_logs()

    def load_memory(self):
        if os.path.exists(self.memory_file):
            with open(self.memory_file, 'r') as file:
                return json.load(file)
        return {}

    def save_memory(self):
        with open(self.memory_file, 'w') as file:
            json.dump(self.memory, file)

    def load_logs(self):
        if os.path.exists(self.log_file):
            with open(self.log_file, 'r') as file:
                return json.load(file)
        return []

    def save_logs(self):
        with open(self.log_file, 'w') as file:
            json.dump(self.logs, file)

    def get_memory(self, user_id):
        return self.memory.get(user_id, {})

    def update_memory(self, user_id, key, value):
        if user_id not in self.memory:
            self.memory[user_id] = {}
        self.memory[user_id][key] = value
        self.save_memory()

    def clear_memory(self, user_id):
        if user_id in self.memory:
            del self.memory[user_id]
            self.save_memory()

    def log_query(self, user_id, query, response):
        timestamp = datetime.datetime.now().isoformat()
        self.logs.append({
            'user_id': user_id,
            'query': query,
            'response': response,
            'timestamp': timestamp
        })
        self.save_logs()






from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

llm = AzureOpenAI(
    model="gpt-35-turbo-16k",
    deployment_name="my-custom-llm",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)

embed_model = AzureOpenAIEmbedding(
    model="text-embedding-ada-002",
    deployment_name="my-custom-embedding",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)

memory = MemoryWithLogging()

def chatbot(user_id, query):
    # Retrieve user's memory
    user_memory = memory.get_memory(user_id)
    
    # Process the query with memory and LLM
    response = llm.generate_response(query, context=user_memory)
    response_text = response['text']

    # Log the query and response
    memory.log_query(user_id, query, response_text)
    
    # Update the memory based on the response
    if 'memory_update' in response:
        for key, value in response['memory_update'].items():
            memory.update_memory(user_id, key, value)
    
    return response_text

# Example usage
user_id = 'user123'
query = "What's the weather like today?"
response = chatbot(user_id, query)
print(response)








class MemoryWithLogging:
    # ... (existing methods)

    def get_logs(self, user_id=None):
        if user_id:
            return [log for log in self.logs if log['user_id'] == user_id]
        return self.logs

# Example usage
logs = memory.get_logs('user123')
for log in logs:
    print(log)

